{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# 1. IMPORT LIBRARIES & MOUNT (if needed)\n",
    "############################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We begin by importing the essential libraries used throughout the project:\n",
    "\n",
    "pandas and numpy for data manipulation and numerical operations. random for picking random values or random choices if needed. seaborn and matplotlib for data visualization."
   ],
   "id": "9dca63672c0de5b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "############################################\n",
    "# 2. LOAD DATASET\n",
    "############################################\n",
    "\n",
    "missing_values = [\n",
    "    \"Not Available\", \"N/A\", \"na\", \"NaN\", \"nan\",\n",
    "    \"NULL\", \"Unknown\", \"--\", \"not available\",\n",
    "    \"unknown\", \"null\", \"\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"/content/Final_corrupted_dataset.csv\", na_values=missing_values)\n",
    "\n",
    "# print(\"Initial dataset shape:\", df.shape)\n",
    "# df.head(10)"
   ],
   "id": "f2bc8dddaf9bd7bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We define a list of missing_values placeholders, such as \"Not Available\", \"N/A\", and \"\". This ensures that when pandas reads the CSV, any cell matching these strings becomes a NaN (missing value). We specify the file_path to our CSV. pd.read_csv loads the data into a DataFrame df. The parameter na_values=missing_values tells pandas to treat those strings as missing. We then print the initial shape of the DataFrame to see how many rows/columns we have, and df.head(10) shows the first 10 rows so we can quickly inspect what the raw data looks like.",
   "id": "d42359db1d03a39d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "############################################\n",
    "# 3. REMOVE DUPLICATES\n",
    "############################################\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"After removing duplicates, shape =\", df.shape)"
   ],
   "id": "a6a3b2683398f648"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "drop_duplicates scans the DataFrame for any exact duplicate rows and removes them. This prevents counting the same record multiple times. reset_index(drop=True) reassigns a new integer index from 0 to len(df)-1. We print the new shape to confirm how many rows remain after duplicates are removed. This helps keep the data unique and consistent.",
   "id": "6fe6b47eb8bbb873"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "############################################\n",
    "# 4. QUICK INSPECTION\n",
    "############################################\n",
    "\n",
    "# print(\"\\n--- INFO ---\")\n",
    "# df.info()\n",
    "\n",
    "# print(\"\\n--- DESCRIBE (include='all') ---\")\n",
    "# display(df.describe(include='all'))\n",
    "\n",
    "# print(\"\\n--- SAMPLE ROWS ---\")\n",
    "# display(df.sample(5))"
   ],
   "id": "74d656984db41df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "df.info(): Summarizes each column’s data type, the number of non-null values, and the overall memory usage. This helps identify columns with missing data or incorrect data types. df.describe(include='all'): Gives a statistical summary for all columns—both numeric (mean, std, min, max) and categorical (count, unique, top, freq). df.sample(5): Displays 5 random rows so we can see the variety of entries. This can reveal unexpected placeholders or anomalies that might not appear in the first few rows.",
   "id": "633cd3fb23021181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "############################################\n",
    "# 5. CONVERT WRITTEN-OUT NUMBERS TO DIGITS\n",
    "############################################\n",
    "\n",
    "word_to_number_dict = {\n",
    "    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4,\n",
    "    \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9,\n",
    "    \"ten\": 10, \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13,\n",
    "    \"fourteen\": 14, \"fifteen\": 15, \"sixteen\": 16,\n",
    "    \"seventeen\": 17, \"eighteen\": 18, \"nineteen\": 19\n",
    "}\n",
    "tens_dict = {\n",
    "    \"twenty\": 20, \"thirty\": 30, \"forty\": 40, \"fifty\": 50,\n",
    "    \"sixty\": 60, \"seventy\": 70, \"eighty\": 80, \"ninety\": 90\n",
    "}\n",
    "\n",
    "def words_to_numbers(value):\n",
    "    \"\"\"Convert English words (like 'sixty-five') to an integer if possible.\"\"\"\n",
    "    if not isinstance(value, str):\n",
    "        return value  # If it's already numeric or NaN, return as-is\n",
    "\n",
    "    word = value.strip().lower()\n",
    "    # Direct 0-19\n",
    "    if word in word_to_number_dict:\n",
    "        return word_to_number_dict[word]\n",
    "    # Direct tens (20,30,...90)\n",
    "    if word in tens_dict:\n",
    "        return tens_dict[word]\n",
    "    # Compound numbers \"twenty-three\", \"ninety-nine\", etc.\n",
    "    if \"-\" in word:\n",
    "        parts = word.split(\"-\")\n",
    "        if len(parts) == 2:\n",
    "            part1, part2 = parts\n",
    "            if part1 in tens_dict and part2 in word_to_number_dict:\n",
    "                return tens_dict[part1] + word_to_number_dict[part2]\n",
    "    # Not recognized => return original\n",
    "    return value\n",
    "\n",
    "# Define numeric columns that might have textual numbers\n",
    "possible_text_nums = [\n",
    "    \"Hours_Studied\", \"Sleep_Hours\", \"Previous_Scores\",\n",
    "    \"Tutoring_Sessions\", \"Physical_Activity\", \"Exam_Score\"\n",
    "]\n",
    "\n",
    "# Converting\n",
    "for col in possible_text_nums:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(words_to_numbers)\n"
   ],
   "id": "5e8c3dddae5ee655"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some numeric columns may contain textual representations (e.g., \"sixty-five\") that should be actual numbers (65). words_to_numbers tries to convert words like \"twenty-three\" into the correct integer. If it can’t, it returns the original value. We apply this function only to columns we suspect might have textual numeric data (like Hours_Studied, Sleep_Hours, etc.). This step ensures we don’t have strings like \"eight\" in numeric columns.",
   "id": "aa1d3b1b8c03953d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "############################################\n",
    "# 6. FORCE NUMERIC COLUMNS\n",
    "############################################\n",
    "\n",
    "# We expect these columns to be numeric\n",
    "numeric_cols = [\n",
    "    \"Hours_Studied\", \"Attendance\", \"Sleep_Hours\",\n",
    "    \"Previous_Scores\", \"Tutoring_Sessions\",\n",
    "    \"Physical_Activity\", \"Exam_Score\"  # If it exists\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        # Convert to numeric, coerce to NaN if unconvertible\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n"
   ],
   "id": "e85bd446a88e9c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We define numeric_cols as columns that we want to be numeric. pd.to_numeric with errors='coerce' attempts to convert each entry to a float. If it can’t (e.g., leftover text), it becomes NaN. This step is critical if the dataset has inconsistent types. By the end, these columns are guaranteed to be numeric or missing.",
   "id": "eec70d50fcc6489b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9377576ca6c65df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
